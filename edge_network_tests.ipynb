{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-03-30 16:33:46,682\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/lucas/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"7\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModel, AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "from swarm.optimizer.edge_optimizer.edge_network import EdgeNetwork\n",
    "from swarm.optimizer.edge_optimizer.parameterization import EdgeWiseDistributionByModel\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "\n",
    "from swarm.llm.custom_llm import CustomLLM\n",
    "from swarm.llm.format import Message\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "from huggingface_hub import login \n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: C\n",
      "Plain Text: The mental nerve is a branch of the trigeminal nerve that contains somatic motor processes.\n"
     ]
    }
   ],
   "source": [
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-23 15:06:50 config.py:618] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 03-23 15:06:50 llm_engine.py:87] Initializing an LLM engine with config: model='google/gemma-7B-it', tokenizer='google/gemma-7B-it', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=5888, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"google/gemma-7B-it\", dtype=\"half\", max_model_len=5888)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7B-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: '<bos><start_of_turn>user\\nWrite a poem about cats.<end_of_turn>\\n<start_of_turn>model\\n', Generated text: \"A purring ball of fluffy delight,\\nWith eyes so bright, and full of light.\\nSoft as velvet, gentle as snow,\\nA heart of gold, a heart of gold.\\n\\nA ball of yarn, a playful toy,\\nPounces and leaps, a merry joy.\\nWith leaps and bounds, they dance with grace,\\nA feline spirit, a wild, sweet space.\\n\\nFrom morning sun to evening stars,\\nThey guard their homes, with watchful bars.\\nA guardian spirit, a protector bold,\\nWith love and loyalty, they unfold.\\n\\nThe heart of a lion, in a tiny frame,\\nA guardian against danger's flame.\\nWith a purr and a gentle head,\\nThey bring joy, never to dread.\\n\\nSo let us cherish these precious souls,\\nWith love and respect, they bring us gold.\\nA gift from the heart, a furry friend,\\nA purring companion, till the end.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "message = [{\"role\": \"user\", \"content\": \"Write a poem about cats.\"}]\n",
    "prompt = tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, top_k=50,max_tokens=2000)\n",
    "outputs = llm.generate(prompt,sampling_params=sampling_params)\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# pip install accelerate\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7B-it\")\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7B-it\",torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7B-it\", torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tokenizer.special_tokens_map)\n",
    "#dummy message\n",
    "message = [{\"role\": \"user\", \"content\": \"Write a poem about cats.\"}]\n",
    "prompt = tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n",
    "prompt = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda\")\n",
    "prompt_len = len(prompt[0])\n",
    "outputs = model.generate(\n",
    "            prompt,\n",
    "            do_sample=True,\n",
    "            max_length=2000,\n",
    "            top_k=50,\n",
    "            top_p=1.0\n",
    "        )\n",
    "output_text = tokenizer.decode(outputs[0][prompt.shape[-1]:],skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A ball of fluff, with purrs so deep,\n",
      "A heart of gold, a gentle leap.\n",
      "Eyes that shine, a emerald glow,\n",
      "A soul to love, forever more.\n",
      "\n",
      "Soft steps in the night so bright,\n",
      "A hunter's instinct, quick as light.\n",
      "Pounce and play, a playful spark,\n",
      "A lion's heart, in the smallest park.\n",
      "\n",
      "A guardian's spirit, strong and true,\n",
      "Protects their own, their family's too.\n",
      "With love and grace, they fill the heart,\n",
      "A furry friend, a gentle start.\n",
      "\n",
      "So let us cherish these creatures sweet,\n",
      "The purring joy that they complete.\n",
      "For in their hearts, a treasure lies,\n",
      "A bond beyond, a love that flies.\n"
     ]
    }
   ],
   "source": [
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using custom LLM class\n",
      "Load Model...\n",
      "WARNING 03-23 15:16:01 config.py:618] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 03-23 15:16:01 llm_engine.py:87] Initializing an LLM engine with config: model='google/gemma-7B-it', tokenizer='google/gemma-7B-it', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=5888, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "INFO 03-23 15:16:05 weight_utils.py:163] Using model weights format ['*.safetensors']\n",
      "INFO 03-23 15:16:09 llm_engine.py:357] # GPU blocks: 430, # CPU blocks: 585\n",
      "INFO 03-23 15:16:11 model_runner.py:684] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 03-23 15:16:11 model_runner.py:688] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 03-23 15:16:17 model_runner.py:756] Graph capturing finished in 6 secs.\n",
      "Load Tokenizer\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the GPTChat class\n",
    "gpt_chat = CustomLLM()\n",
    "#gpt_chat_2 = CustomLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:07<00:00,  7.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A dog\\'s spoken language would likely depend on the breed, individual personality, and training. However, some common sounds and phrases a dog might say if they could speak include:\\n\\n**Basic sounds:**\\n\\n* **Bark:** To warn of danger, protect their territory, or express excitement.\\n* **Whine:** To express pain, discomfort, or submission.\\n* **Growl:** To warn of threat, establish dominance, or protect themselves.\\n* **Hiss:** To express fear, threat, or discomfort.\\n* **Click:** To mark a positive behavior or to signal understanding.\\n\\n**Basic phrases:**\\n\\n* **\"Woof!\":** To greet people, express joy, or mark their presence.\\n* **\"Boo!\":** To warn of danger or to startle someone.\\n* **\"Fetch!\":** To invite play or to request a game.\\n* **\"Play!\":** To express desire to play or have fun.\\n* **\"Treat!\":** To signal that they want a treat.\\n* **\"Home!\":** To signal that they want to go home.\\n* **\"Mine!\":** To claim ownership of a toy or object.\\n\\n**More complex phrases:**\\n\\n* **\"I love you!\":** To express affection or loyalty.\\n* **\"Thank you!\":** To express gratitude.\\n* **\"Sorry!\":** To apologize for their actions.\\n* **\"Help me!\":** To ask for assistance.\\n* **\"What\\'s up?\":** To inquire about their surroundings or people.\\n* **\"I\\'m hungry!\":** To signal their need for food.\\n* **\"I\\'m tired:** To express tiredness or exhaustion.\\n\\nIt is important to note that these are just some possible sounds and phrases a dog might make if they could speak. The specific sounds and phrases would vary based on the individual dog\\'s personality and training.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "#if gpt_chat is gpt_chat_2:\n",
    "#    print(\"Same instance\")\n",
    "# Create a list of Message objects\n",
    "messages = [Message(role=\"user\", content=\"What would a dog say if he could speak?\")]\n",
    "# Move messages to GPU\n",
    "# Create tasks for the gen method\n",
    "tasks = [asyncio.create_task(gpt_chat.agen(messages))]#, asyncio.create_task(gpt_chat_2.agen(messages))]\n",
    "\n",
    "# Wait for the tasks to complete and get the results\n",
    "results = [await task for task in tasks]\n",
    "\n",
    "# Print the output\n",
    "print(results)\n",
    "#print(results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_connections = [0 for _ in range(188)]\n",
    "init_connection_probability = 0.1\n",
    "domain = \"crosswords\"\n",
    "llm_backbone_name=\"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Swarm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m swarm \u001b[38;5;241m=\u001b[39m \u001b[43mSwarm\u001b[49m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrosswordsReflection\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrosswordsToT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrosswordsBruteForceOpt\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrosswords\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo-1106\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m#\"gpt-4-1106-preview\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m             final_node_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturnAll\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      3\u001b[0m             final_node_kwargs\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m      4\u001b[0m             edge_optimize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m             init_connection_probability\u001b[38;5;241m=\u001b[39minit_connection_probability, \n\u001b[1;32m      6\u001b[0m             connect_output_nodes_to_final_node\u001b[38;5;241m=\u001b[39mconnect_output_nodes_to_final_node, \n\u001b[1;32m      7\u001b[0m             include_inner_agent_connections\u001b[38;5;241m=\u001b[39minclude_inner_agent_connections,\n\u001b[1;32m      8\u001b[0m             edge_network_enable\u001b[38;5;241m=\u001b[39medge_network_enable,\n\u001b[1;32m      9\u001b[0m             llm_backbone_name\u001b[38;5;241m=\u001b[39mllm_backbone_name)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Swarm' is not defined"
     ]
    }
   ],
   "source": [
    "swarm = Swarm([\"CrosswordsReflection\", \"CrosswordsToT\", \"CrosswordsBruteForceOpt\"], \"crosswords\", \"gpt-3.5-turbo-1106\", #\"gpt-4-1106-preview\"\n",
    "            final_node_class=\"ReturnAll\", \n",
    "            final_node_kwargs={},\n",
    "            edge_optimize=True,\n",
    "            init_connection_probability=init_connection_probability, \n",
    "            connect_output_nodes_to_final_node=connect_output_nodes_to_final_node, \n",
    "            include_inner_agent_connections=include_inner_agent_connections,\n",
    "            edge_network_enable=edge_network_enable,\n",
    "            llm_backbone_name=llm_backbone_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m edge_network \u001b[38;5;241m=\u001b[39m EdgeNetwork(llm_backbone_name\u001b[38;5;241m=\u001b[39mllm_backbone_name, num_edges\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(potential_connections), initial_probability\u001b[38;5;241m=\u001b[39minit_connection_probability)\n\u001b[0;32m----> 2\u001b[0m connection_dist \u001b[38;5;241m=\u001b[39m \u001b[43mEdgeWiseDistributionByModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpotential_connections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdomain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m connection_dist\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult/crosswords/experiment_edge_logits_10.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/thesis/DynamicGPTSwarm/swarm/optimizer/edge_optimizer/parameterization.py:159\u001b[0m, in \u001b[0;36mEdgeWiseDistributionByModel.__init__\u001b[0;34m(self, potential_connections, model, domain)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(potential_connections)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 159\u001b[0m node_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([x \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m potential_connections \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m pair])\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_idx2id \u001b[38;5;241m=\u001b[39m {i: node_id \u001b[38;5;28;01mfor\u001b[39;00m i, node_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(node_ids)}\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_id2idx \u001b[38;5;241m=\u001b[39m {node_id: i \u001b[38;5;28;01mfor\u001b[39;00m i, node_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(node_ids)}\n",
      "File \u001b[0;32m~/thesis/DynamicGPTSwarm/swarm/optimizer/edge_optimizer/parameterization.py:159\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(potential_connections)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 159\u001b[0m node_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([x \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m potential_connections \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m pair])\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_idx2id \u001b[38;5;241m=\u001b[39m {i: node_id \u001b[38;5;28;01mfor\u001b[39;00m i, node_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(node_ids)}\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_id2idx \u001b[38;5;241m=\u001b[39m {node_id: i \u001b[38;5;28;01mfor\u001b[39;00m i, node_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(node_ids)}\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "edge_network = EdgeNetwork(llm_backbone_name=llm_backbone_name, num_edges=len(potential_connections), initial_probability=init_connection_probability)\n",
    "connection_dist = EdgeWiseDistributionByModel(potential_connections, edge_network, domain)\n",
    "\n",
    "connection_dist.load_state_dict(torch.load(\"result/crosswords/experiment_edge_logits_10.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of EdgeNetwork\n",
    "num_edges = 188  # Replace with the number of edges used when saving the model\n",
    "llm_backbone_name = 'gpt2'  # Replace with the name of the model used when saving the model\n",
    "model = EdgeNetwork(llm_backbone_name, num_edges)\n",
    "\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load('result/crosswords/experiment_edge_logits_10.pt')\n",
    "\n",
    "# Remove 'model.' prefix from state dictionary keys and exclude 'order_params'\n",
    "state_dict = {k.replace('model.', ''): v for k, v in state_dict.items() if k != 'model.order_params'}\n",
    "\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of EdgeNetwork\n",
    "num_edges = 188  # Replace with the number of edges used when saving the model\n",
    "llm_backbone_name = 'gpt2'  # Replace with the name of the model used when saving the model\n",
    "model = EdgeNetwork(llm_backbone_name, num_edges)\n",
    "\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load('result/crosswords/experiment_edge_logits_10.pt')\n",
    "\n",
    "# Remove 'model.' prefix from state dictionary keys\n",
    "state_dict = {k.replace('model.', ''): v for k, v in state_dict.items()}\n",
    "\n",
    "model.load_state_dict(state_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weak_to_strong",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
