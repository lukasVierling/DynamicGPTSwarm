{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/lucas/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"4,6,7\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModel, AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "from swarm.optimizer.edge_optimizer.edge_network import EdgeNetwork\n",
    "from swarm.optimizer.edge_optimizer.parameterization import EdgeWiseDistributionByModel\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from swarm.graph.swarm import Swarm\n",
    "from typing import List, Any, Optional\n",
    "\n",
    "from swarm.llm.custom_llm import CustomLLM\n",
    "from swarm.llm.format import Message\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from experiments.evaluator.datasets.mmlu_dataset import MMLUDataset\n",
    "from experiments.evaluator.datasets.cmmlu_dataset import CMMLUDataset\n",
    "from experiments.evaluator.datasets.mixedmmlu_dataset import MixedMMLUDataset\n",
    "import pickle\n",
    "import json\n",
    "import asyncio\n",
    "import json\n",
    "from huggingface_hub import login \n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import io\n",
    "\n",
    "class CPU_Unpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
    "            return lambda b: torch.load(io.BytesIO(b), map_location='cuda:7')\n",
    "        else:\n",
    "            return super().find_class(module, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixedmmlu_8true_0adv_edge_iter200_domain_mixedmmlu_MajorityVote_OptimizedSwarm_1714788267.6306956.pkl\n",
      "0.524\n"
     ]
    }
   ],
   "source": [
    "path = \"mixedmmlu_8true_0adv_edge_iter200_domain_mixedmmlu_MajorityVote_OptimizedSwarm_1714788267.6306956\" + \".pkl\"\n",
    "with open(f\"result/mmlu/{path}\", 'rb') as f:\n",
    "    swarm = pickle.load(f)  # Load the swarm\n",
    "    #swarm = CPU_Unpickler(f).load()\n",
    "    #delete swarm object and clear memory\n",
    "    score = pickle.load(f)  # Load the score\n",
    "print(path)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "for i,node in enumerate(swarm.composite_graph.nodes.values()):\n",
    "    if i ==0:\n",
    "        continue\n",
    "    print(node.llm.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('edge_logits', tensor([-0.9060, -2.0771, -0.3538, -1.9184, -2.3243, -3.0701, -3.8445, -3.8105,\n",
      "        -2.0401, -1.5536, -2.3212, -4.7880, -1.9222, -1.7154, -4.7214, -2.5089,\n",
      "        -2.1340,  0.9295, -2.5914,  1.2137, -2.2820, -2.1987, -3.1128,  0.2236,\n",
      "        -1.2063, -2.0589, -4.0054, -1.2779, -2.5772, -4.5234, -2.2227, -1.8173,\n",
      "        -3.7507, -2.4876, -0.0229, -2.1858])), ('order_params', tensor([ 0.2713, -1.2729,  0.5027,  0.4181, -0.6394, -0.6608, -0.1433, -0.1043,\n",
      "        -1.5313]))])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(f\"result/crosswords/old_method_10_it_nostuck_20batch/experiment8_edge_logits_9.pt\", 'rb') as f:\n",
    "    edge_logits = torch.load(f)  # Load the swarm\n",
    "    #delete swarm object and clear memory\n",
    "print(edge_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using custom LLM class, model_name: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "Load Model...\n",
      "Selected GPU: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.64s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m swarm \u001b[38;5;241m=\u001b[39m \u001b[43mSwarm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCrosswordsBruteForceOpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCrosswordsReflection\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcrosswords\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Meta-Llama-3-8B-Instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#\"google/gemma-7B-it\",#,#\"gpt-3.5-turbo-1106\", #\"gpt-4-1106-preview\" ,  #\"CrosswordsToT\",\"CrosswordsBruteForceOpt\",\"CrosswordsReflection\"\u001b[39;49;00m\n\u001b[1;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfinal_node_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReturnAll\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfinal_node_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43medge_optimize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43minit_connection_probability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconnect_output_nodes_to_final_node\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43minclude_inner_agent_connections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43medge_network_enable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mllm_backbone_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m swarm\u001b[38;5;241m.\u001b[39mconnection_dist\u001b[38;5;241m.\u001b[39medge_logits \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(edge_logits[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_logits\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/DynamicGPTSwarm/swarm/graph/swarm.py:63\u001b[0m, in \u001b[0;36mSwarm.__init__\u001b[0;34m(self, agent_names, domain, model_name, open_graph_as_html, final_node_class, final_node_kwargs, edge_optimize, node_optimize, init_connection_probability, connect_output_nodes_to_final_node, include_inner_agent_connections, edge_network_enable, llm_backbone_name, price_list)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_backbone_name \u001b[38;5;241m=\u001b[39m llm_backbone_name\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprice_list \u001b[38;5;241m=\u001b[39m price_list\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morganize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude_inner_agent_connections\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DynamicGPTSwarm/swarm/graph/swarm.py:68\u001b[0m, in \u001b[0;36mSwarm.organize\u001b[0;34m(self, include_inner_agent_connections)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21morganize\u001b[39m(\u001b[38;5;28mself\u001b[39m, include_inner_agent_connections: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mused_agents \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 68\u001b[0m     decision_method \u001b[38;5;241m=\u001b[39m \u001b[43mOperationRegistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinal_node_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdomain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinal_node_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomposite_graph \u001b[38;5;241m=\u001b[39m CompositeGraph(decision_method,\n\u001b[1;32m     70\u001b[0m                                           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name)\n\u001b[1;32m     71\u001b[0m     potential_connections \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/DynamicGPTSwarm/swarm/environment/operations/operation_registry.py:20\u001b[0m, in \u001b[0;36mOperationRegistry.get\u001b[0;34m(cls, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mcls\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Node:\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/class_registry/registry.py:114\u001b[0m, in \u001b[0;36mBaseRegistry.get\u001b[0;34m(self, key, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     95\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    Creates a new instance of the class matching the specified key.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m      - :py:meth:`__init__`\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/class_registry/registry.py:145\u001b[0m, in \u001b[0;36mBaseRegistry.create_instance\u001b[0;34m(class_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_instance\u001b[39m(class_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# type: (type, *Any, **Any) -> Any\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Prepares the return value for :py:meth:`get`.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m        Keyword arguments passed to :py:meth:`get`.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclass_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DynamicGPTSwarm/swarm/environment/operations/crosswords/return_all.py:24\u001b[0m, in \u001b[0;36mReturnAll.__init__\u001b[0;34m(self, domain, model_name, operation_description, id, best_state)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(operation_description, \u001b[38;5;28mid\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain \u001b[38;5;241m=\u001b[39m domain\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;241m=\u001b[39m \u001b[43mLLMRegistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_set \u001b[38;5;241m=\u001b[39m PromptSetRegistry\u001b[38;5;241m.\u001b[39mget(domain)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_state \u001b[38;5;241m=\u001b[39m best_state\n",
      "File \u001b[0;32m~/DynamicGPTSwarm/swarm/llm/llm_registry.py:30\u001b[0m, in \u001b[0;36mLLMRegistry.get\u001b[0;34m(cls, model_name)\u001b[0m\n\u001b[1;32m     27\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregistry\u001b[38;5;241m.\u001b[39mget(model_name)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m CUSTOM_LLMS:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# for custom model get the smalle gemma model and run on our gpus\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCustomLLM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# any version of GPTChat like \"gpt-4-1106-preview\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregistry\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPTChat\u001b[39m\u001b[38;5;124m'\u001b[39m, model_name)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/class_registry/registry.py:114\u001b[0m, in \u001b[0;36mBaseRegistry.get\u001b[0;34m(self, key, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     95\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    Creates a new instance of the class matching the specified key.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m      - :py:meth:`__init__`\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/class_registry/registry.py:145\u001b[0m, in \u001b[0;36mBaseRegistry.create_instance\u001b[0;34m(class_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_instance\u001b[39m(class_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# type: (type, *Any, **Any) -> Any\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Prepares the return value for :py:meth:`get`.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m        Keyword arguments passed to :py:meth:`get`.\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mclass_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DynamicGPTSwarm/swarm/llm/custom_llm.py:82\u001b[0m, in \u001b[0;36mCustomLLM.__init__\u001b[0;34m(self, model_name)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (CustomLLM\u001b[38;5;241m.\u001b[39mMAX_LLMS):\n\u001b[1;32m     81\u001b[0m         CustomLLM\u001b[38;5;241m.\u001b[39mdevices[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name][i] \u001b[38;5;241m=\u001b[39m select_gpu()\n\u001b[0;32m---> 82\u001b[0m         CustomLLM\u001b[38;5;241m.\u001b[39mmodels[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name][i] \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mCustomLLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m CustomLLM\u001b[38;5;241m.\u001b[39mtokenizers:\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoad Tokenizer...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/gptswarm/lib/python3.10/site-packages/transformers/modeling_utils.py:2556\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2552\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2554\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2555\u001b[0m         )\n\u001b[0;32m-> 2556\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gptswarm/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gptswarm/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gptswarm/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gptswarm/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/gptswarm/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "swarm = Swarm([\"CrosswordsBruteForceOpt\",\"CrosswordsReflection\"], \"crosswords\", \"meta-llama/Meta-Llama-3-8B-Instruct\",#\"google/gemma-7B-it\",#,#\"gpt-3.5-turbo-1106\", #\"gpt-4-1106-preview\" ,  #\"CrosswordsToT\",\"CrosswordsBruteForceOpt\",\"CrosswordsReflection\"\n",
    "            final_node_class=\"ReturnAll\", \n",
    "            final_node_kwargs={},\n",
    "            edge_optimize=True,\n",
    "            init_connection_probability=0.1, \n",
    "            connect_output_nodes_to_final_node=True, \n",
    "            include_inner_agent_connections=True,\n",
    "            edge_network_enable=False,\n",
    "            llm_backbone_name=\"\")\n",
    "swarm.connection_dist.edge_logits = nn.Parameter(edge_logits['edge_logits'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph, _ = swarm.connection_dist.realize(swarm.composite_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<swarm.graph.composite_graph.CompositeGraph object at 0x77b3347eda80>\n"
     ]
    }
   ],
   "source": [
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swarm.graph.visualize import GPTSwarmVis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "/home/lucas/DynamicGPTSwarm/result/example.html\n"
     ]
    }
   ],
   "source": [
    "\n",
    "GPTSwarmVis(graph, file_name = \"result.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from swarm.graph.swarm import Swarm\n",
    "from swarm.environment.operations.final_decision import MergingStrategy\n",
    "from experiments.evaluator.evaluator import Evaluator\n",
    "from experiments.evaluator.datasets.mmlu_dataset import MMLUDataset\n",
    "from dataset.MMLU.download import download\n",
    "from experiments.evaluator.datasets.cmmlu_dataset import CMMLUDataset\n",
    "from experiments.evaluator.datasets.mixedmmlu_dataset import MixedMMLUDataset\n",
    "from dataset.CMMLU.download import download as cmmlu_download\n",
    "\n",
    "from swarm.llm.custom_llm import CustomLLM\n",
    "\n",
    "swarm = Swarm(\n",
    "    [\"IO\"],\n",
    "    \"mmlu\",\n",
    "    model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    final_node_class=\"FinalDecision\",\n",
    "    final_node_kwargs=dict(strategy=MergingStrategy.MajorityVote),\n",
    "    edge_optimize=True,\n",
    "    edge_network_enable=False,\n",
    "    llm_backbone_name=\"google/gemma-2B\",\n",
    ")\n",
    "print(swarm.connection_dist.edge_logits.shape)\n",
    "#got the swarm, now evaluator\n",
    "dataset_train = MMLUDataset('dev', categories=[\"college_mathematics\",\"elementary_mathematics\",\"formal_logic\", \"abstract_algebra\", \"high_school_mathematics\"])\n",
    "dataset_val = MMLUDataset('val',categories=[\"college_mathematics\",\"elementary_mathematics\",\"formal_logic\", \"abstract_algebra\", \"high_school_mathematics\"])\n",
    "evaluator = Evaluator(\n",
    "    swarm,\n",
    "    dataset_val,\n",
    "    dataset_val,\n",
    "    model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    enable_tensorboard = False,\n",
    "    enable_artifacts=True,\n",
    "    tensorboard_tag=None)\n",
    "accs = {}\n",
    "accs['IO'] = await evaluator.evaluate_agent(agent=\"IO\", limit_questions=25)\n",
    "accs['COT'] = await evaluator.evaluate_agent(agent=\"COT\", limit_questions=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = [0.1, 0.3, 0.1, 0.1, 0.1, 0.2, 0.2, 0.3, 0.1, 0.4, 0.1, 0.2, 0.2, 0.3, 0.1, 0.0, 0.1, 0.0, 0.1, 0.1, 0.1, 0.1, 0.2, 0.1, 0.0, 0.4, 0.2, 0.1, 0.1, 0.2, 0.4, 0.2, 0.2, 0.1, 0.2, 0.2, 0.1, 0.2, 0.2, 0.2, 0.2, 0.2, 0.1, 0.1, 0.0, 0.4, 0.2, 0.1, 0.0, 0.1, 0.1, 0.2, 0.3, 0.4, 0.5, 0.1, 0.1, 0.2, 0.1, 0.1, 0.1, 0.1, 0.0, 0.3, 0.1, 0.4, 0.1, 0.1, 0.2, 0.0, 0.0, 0.3, 0.2, 0.3, 0.1, 0.2, 0.4, 0.1, 0.2, 0.0, 0.2, 0.2, 0.0, 0.2, 0.1]\n",
    "\n",
    "data = [[i for i in data[j:j+20]]for j in range(len(data)//20)]\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = torch.nn.Linear(2048,36)\n",
    "path = \"result/crosswords/experiment8_edge_logits_0.pt\"\n",
    "dict = torch.load(path)\n",
    "for key in dict.keys():\n",
    "    print(key)\n",
    "    print(dict[key].shape)\n",
    "    print(dict[key])\n",
    "    print(\"----\")\n",
    "lin.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result/crosswords/experiment9_edge_network-False_final_utilities.pkl', 'rb') as f:\n",
    "    utilities = pkl.load(f)\n",
    "print(utilities.keys())\n",
    "for key in utilities.keys():\n",
    "    if key !=\"utilities\":\n",
    "        print(key, \" \", utilities[key])\n",
    "with open('result/crosswords/experiment9_edge_network-True_final_utilities.pkl', 'rb') as f:\n",
    "    utilities = pkl.load(f)\n",
    "print(utilities.keys())\n",
    "for key in utilities.keys():\n",
    "    if key !=\"utilities\":\n",
    "        print(key, \" \", utilities[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result/crosswords/experiment9_edge_network-False_final_utilities.pkl', 'rb') as f:\n",
    "    utilities = pkl.load(f)\n",
    "print(utilities.keys())\n",
    "for key in utilities.keys():\n",
    "    if key !=\"utilities\":\n",
    "        print(key, \" \", utilities[key])\n",
    "\n",
    "#turn every mean into max 3 decimals after comma\n",
    "\n",
    "utilities['mean_utilities_per_run'] = [round(i,3) for i in utilities['mean_utilities_per_run']]\n",
    "utilities['overall_mean'] = round(np.mean(utilities['mean_utilities_per_run'], axis=0),3)\n",
    "utilities['overall_std'] = round(np.std(utilities['mean_utilities_per_run'], axis=0),3)\n",
    "print(utilities['overall_mean'])\n",
    "print(utilities['overall_std'])\n",
    "print(utilities['mean_utilities_per_run'])\n",
    "#dump results\n",
    "with open('result/crosswords/experiment9_edge_network-False_final_utilities.pkl', 'wb') as f:\n",
    "    pkl.dump(utilities, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = CrosswordsEvaluator(test_data, batch_size=batch_size, metric=\"words\", window_size=num_batches)\n",
    "swarm = Swarm([\"CrosswordsBruteForceOpt\",\"CrosswordsReflection\"], \"crosswords\", \"meta-llama/Meta-Llama-3-8B-Instruct\",#\"google/gemma-7B-it\",#,#\"gpt-3.5-turbo-1106\", #\"gpt-4-1106-preview\" ,  #\"CrosswordsToT\",\"CrosswordsBruteForceOpt\",\"CrosswordsReflection\"\n",
    "            final_node_class=\"ReturnAll\", \n",
    "            final_node_kwargs={},\n",
    "            edge_optimize=True,\n",
    "            init_connection_probability=init_connection_probability, \n",
    "            connect_output_nodes_to_final_node=True, \n",
    "            include_inner_agent_connections=True,\n",
    "            edge_network_enable=edge_network_enable,\n",
    "            llm_backbone_name=llm_backbone_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_messages(messages: List[Message]) -> List[Message]:\n",
    "        processed_messages = []\n",
    "        system_message = None\n",
    "\n",
    "        for message in messages:\n",
    "            if message.role == 'system':\n",
    "                system_message = message.content\n",
    "            elif message.role == 'user':\n",
    "                if system_message:\n",
    "                    message.content = system_message + ' ' + message.content\n",
    "                    system_message = None\n",
    "                processed_messages.append(message)\n",
    "\n",
    "        # Handle the case where the last message is a system message\n",
    "        if system_message:\n",
    "            processed_messages.append(Message(role='user', content=system_message))\n",
    "\n",
    "        return processed_messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load gemma-7B-it with Custom LLM class\n",
    "\n",
    "from swarm.environment.prompt.prompt_set_registry import PromptSetRegistry\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = CustomLLM(\"google/gemma-7B-it\" )\n",
    "test_dataset = CMMLUDataset(\"test\")\n",
    "prompt_set = PromptSetRegistry.get(\"cmmlu\")\n",
    "\n",
    "#test_dataset._total_df = test_dataset._total_df[:20]\n",
    "\n",
    "tqdm = tqdm(test_dataset)\n",
    "\n",
    "\n",
    "y = []\n",
    "\n",
    "for sample in tqdm:\n",
    "    input = test_dataset.record_to_swarm_input(sample)\n",
    "    role = prompt_set.get_role()\n",
    "    constraint = prompt_set.get_constraint()\n",
    "    message = [Message(role=\"system\", content=f\"You are a {role}. {constraint}\"),Message(role=\"user\", content=input[\"task\"])]\n",
    "\n",
    "    input_dict = process_messages(message)\n",
    "    answer = model.gen(message)\n",
    "    answer = test_dataset.postprocess_answer(answer)\n",
    "    correct_answer = test_dataset.record_to_target_answer(sample)\n",
    "    y.append(answer == correct_answer)\n",
    "print(\"final accuracy: \", np.mean(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_chat_template(messages, add_generation_prompt=True):\n",
    "    if add_generation_prompt:\n",
    "        if messages[-1][\"role\"] == \"user\":\n",
    "            messages.append({\"role\": \"assistant\", \"content\": \"\"})\n",
    "    chat = \"\"\n",
    "    for message in messages:\n",
    "        print(message)\n",
    "        role, content = message.values()\n",
    "        print(role, content)\n",
    "        if role == \"user\":\n",
    "            chat += f\"[|Human|]:{content}\"\n",
    "        elif role == \"assistant\":\n",
    "            chat += f\"[|AI|]:{content}\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown role {role}\")\n",
    "    return chat\n",
    "tokenizer.apply_chat_template = apply_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"result/crosswords/old_method_10_it_nostuck_20batch/experiment8_utilities_9.pkl\", \"rb\") as f:\n",
    "    utilities = pkl.load(f)\n",
    "#print(utilities[100:])\n",
    "print(np.mean(utilities[100:]))\n",
    "for i in range(0,200,20):\n",
    "    print(utilities[i:i+20])\n",
    "    print(np.mean(utilities[i:i+20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.sigmoid(swarm['edge_logits']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adjacency_matrix(edge_probs, swarm):\n",
    "    assert swarm is not None\n",
    "\n",
    "    # Create an empty adjacency matrix\n",
    "    num_nodes = len(swarm.composite_graph.nodes)\n",
    "    adjacency_matrix = np.zeros((num_nodes, num_nodes))\n",
    "\n",
    "    # Create a dictionary to map indices to nodes\n",
    "    node_indices = {}\n",
    "    node_names = {}\n",
    "    for index, (node_id, node_ref) in enumerate(swarm.composite_graph.nodes.items()):\n",
    "        node_indices[node_id] = index\n",
    "        node_names[node_id] = node_ref.node_name\n",
    "\n",
    "    # Iterate over the connections and probabilities\n",
    "    for conn, prob in zip(swarm.connection_dist.potential_connections, edge_probs):\n",
    "        src_id, dst_id = conn\n",
    "        src_index = node_indices[src_id]\n",
    "        dst_index = node_indices[dst_id]\n",
    "        adjacency_matrix[src_index, dst_index] = prob.item()\n",
    "\n",
    "    return adjacency_matrix, node_indices, node_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print_conns(edge_probs: torch.Tensor, swarm):\n",
    "    assert swarm is not None\n",
    "    msgs = []\n",
    "    for i_conn, (conn, prob) in enumerate(zip(\n",
    "            swarm.connection_dist.potential_connections, edge_probs)):\n",
    "        src_id, dst_id = conn\n",
    "        src_node = swarm.composite_graph.find_node(src_id)\n",
    "        dst_node = swarm.composite_graph.find_node(dst_id)\n",
    "        src_node_name =  src_node.model_name if hasattr(src_node,\"model_name\") else src_node.node_name\n",
    "        dst_node_name = dst_node.model_name if hasattr(dst_node,\"model_name\") else dst_node.node_name#\n",
    "        msg = (f\"{i_conn}: src={src_node_name}({src_node.id}), \"\n",
    "                f\"dst={dst_node_name}({dst_node.id}), prob={prob.item():.3f}\")\n",
    "        msgs.append(msg+\"\\n\")\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math = {'question': ['Let A be the set of all ordered pairs of integers (m, n) such that 7m + 12n = 22. What is the greatest negative number in the set B = {m + n : (m, n) \\in A}?'],\n",
    "'A': ['-5'],\n",
    "'B': ['-4'],\n",
    "'C': ['-3'],\n",
    "'D': ['-2'],\n",
    "'Solution': ['B']\n",
    "}\n",
    "logic = {\n",
    "    'question': ['Construct a complete truth table for the following pairs of propositions. Then, using the truth tables, determine whether the statements are logically equivalent or contradictory. If neither, determine whether they are consistent or inconsistent. Justify your answers. E ⊃ (F · E) and ~E · F'],\n",
    "    'A': ['Logically equivalent'],\n",
    "    'B': ['Contradictory'],\n",
    "    'C': ['Neither logically equivalent nor contradictory, but consistent'],\n",
    "    'D': ['Inconsistent'],\n",
    "    'Solution': ['C']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics:  67\n",
      "Total number of questions:  11649\n",
      "Total df length: 11649\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:7 and cuda:1! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m input_dict \u001b[38;5;241m=\u001b[39m dataset_val\u001b[38;5;241m.\u001b[39mrecord_to_swarm_input(record)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 20\u001b[0m     edge_probs \u001b[38;5;241m=\u001b[39m \u001b[43mswarm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_edge_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mswarm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomposite_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Move edge_probs to CPU memory\u001b[39;00m\n\u001b[1;32m     23\u001b[0m edge_probs \u001b[38;5;241m=\u001b[39m edge_probs\u001b[38;5;241m.\u001b[39mcpu()\n",
      "File \u001b[0;32m~/DynamicGPTSwarm/swarm/optimizer/edge_optimizer/parameterization.py:176\u001b[0m, in \u001b[0;36mEdgeWiseDistributionByModel.get_edge_probs\u001b[0;34m(self, graph, inputs)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmmlu\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmixedmmlu\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcmmlu\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    175\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m [inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/gptswarm/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gptswarm/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/DynamicGPTSwarm/swarm/optimizer/edge_optimizer/edge_network.py:42\u001b[0m, in \u001b[0;36mEdgeNetwork.forward\u001b[0;34m(self, input_text)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_text):\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# Move input to GPU\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mbatch_encode_plus(input_text, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 42\u001b[0m     llm_bare_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_backbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     llm_output \u001b[38;5;241m=\u001b[39m llm_bare_output\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m     44\u001b[0m     llm_output \u001b[38;5;241m=\u001b[39m llm_output[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/gptswarm/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gptswarm/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gptswarm/lib/python3.10/site-packages/transformers/models/gemma/modeling_gemma.py:860\u001b[0m, in \u001b[0;36mGemmaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    857\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 860\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m past_seen_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:  \u001b[38;5;66;03m# kept for BC (cache positions)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gptswarm/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gptswarm/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gptswarm/lib/python3.10/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gptswarm/lib/python3.10/site-packages/torch/nn/functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:7 and cuda:1! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "dataset_val = CMMLUDataset('test')\n",
    "print(\"Total df length:\", len(dataset_val._total_df))\n",
    "#dataset_val._total_df = dataset_val._total_df[:3000]\n",
    "all_probs = []\n",
    "swarm.connection_dist.model.eval()\n",
    "\n",
    "# Initialize list to store all edge_probs\n",
    "all_edge_probs = []\n",
    "swarm.connection_dist.model.eval()\n",
    "\n",
    "for i, record in enumerate(dataset_val):\n",
    "    record = pd.DataFrame([record])\n",
    "    input_dict = dataset_val.record_to_swarm_input(record)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        edge_probs = swarm.connection_dist.get_edge_probs(swarm.composite_graph, inputs=input_dict)\n",
    "    \n",
    "    # Move edge_probs to CPU memory\n",
    "    edge_probs = edge_probs.cpu()\n",
    "    \n",
    "    all_edge_probs.append(edge_probs)\n",
    "\n",
    "\n",
    "# Stack all edge_probs into a tensor\n",
    "all_edge_probs = torch.stack(all_edge_probs)\n",
    "\n",
    "# Calculate mean edge_probs\n",
    "mean_edge_probs = torch.mean(all_edge_probs, dim=0)\n",
    "'''\n",
    "# Initialize list to store distances and corresponding records\n",
    "distances = []\n",
    "\n",
    "for record in dataset_val:\n",
    "    record_copy = copy.deepcopy(record)\n",
    "    record_df = pd.DataFrame([record_copy])\n",
    "    input_dict = dataset_val.record_to_swarm_input(record_df)\n",
    "    edge_probs = swarm.connection_dist.get_edge_probs(swarm.composite_graph, inputs=input_dict)\n",
    "    \n",
    "    # Calculate distance to the mean\n",
    "    distance = torch.norm(edge_probs - mean_edge_probs)\n",
    "    \n",
    "    # Store distance and record\n",
    "    distances.append((distance, record_copy, edge_probs))\n",
    "# Sort distances in descending order and keep top 10\n",
    "top_10_records = sorted(distances, key=lambda x: x[0], reverse=True)[:10]\n",
    "\n",
    "# Extract records from tuples\n",
    "top_10_records = [(record, edge_probs) for _, record, edge_probs  in top_10_records]\n",
    "print(\"top10:\")\n",
    "for record,probs in top_10_records:\n",
    "    print(\"Question: \", record)\n",
    "    _print_conns(probs, swarm)\n",
    "    print(\"Subtracting the mean: \")\n",
    "    _print_conns(mean_edge_probs-probs ,swarm)\n",
    "    print(\"----NExt-----\")\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record,probs in top_10_records:\n",
    "    print(\"Question: \", record)\n",
    "    _print_conns(probs, swarm)\n",
    "    print(\"Subtracting the mean: \")\n",
    "    _print_conns(mean_edge_probs-probs ,swarm)\n",
    "    print(\"----NExt-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0016, 0.8864, 0.2510, 0.0700, 0.0464, 0.3578, 0.7493, 0.3007, 0.7684,\n",
      "        0.5983, 0.9801, 0.2579, 0.9183, 0.3974, 0.2133, 0.2039, 0.4744, 0.1440,\n",
      "        0.0115, 0.7529, 0.2006, 0.8509, 0.0492, 0.9855, 0.1580, 0.8205, 0.9010,\n",
      "        0.3289, 0.1866, 0.9556, 0.8487, 0.0757, 0.7912, 0.7321, 0.9856, 0.9899,\n",
      "        0.7145, 0.7786, 0.0045, 0.8227, 0.2271, 0.8237, 0.1396, 0.1561, 0.2381,\n",
      "        0.0278, 0.9380, 0.2694, 0.9408, 0.3368, 0.9891, 0.9213, 0.1622, 0.4483,\n",
      "        0.0269, 0.0035, 0.8168, 0.5094, 0.6646, 0.5575, 0.4492, 0.9254, 0.0391,\n",
      "        0.8257])\n",
      "tensor([0.0005, 0.0281, 0.0504, 0.0094, 0.0102, 0.0504, 0.0414, 0.0423, 0.0465,\n",
      "        0.0516, 0.0025, 0.0335, 0.0203, 0.0546, 0.0378, 0.0256, 0.0306, 0.0223,\n",
      "        0.0020, 0.0359, 0.0312, 0.0304, 0.0109, 0.0031, 0.0241, 0.0489, 0.0145,\n",
      "        0.0336, 0.0346, 0.0088, 0.0274, 0.0080, 0.0224, 0.0300, 0.0040, 0.0033,\n",
      "        0.0374, 0.0341, 0.0013, 0.0299, 0.0422, 0.0282, 0.0198, 0.0425, 0.0389,\n",
      "        0.0056, 0.0135, 0.0360, 0.0120, 0.0433, 0.0024, 0.0176, 0.0267, 0.0435,\n",
      "        0.0046, 0.0007, 0.0320, 0.0405, 0.0414, 0.0600, 0.0485, 0.0148, 0.0117,\n",
      "        0.0195])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "Mean: \n",
      "0: src=google/gemma-7B-it(3dM7), dst=vivo-ai/BlueLM-7B-Chat(32b8), prob=0.002\n",
      "1: src=google/gemma-7B-it(3dM7), dst=google/gemma-7B-it(7Faz), prob=0.878\n",
      "2: src=google/gemma-7B-it(3dM7), dst=vivo-ai/BlueLM-7B-Chat(7gQh), prob=0.247\n",
      "3: src=google/gemma-7B-it(3dM7), dst=google/gemma-7B-it(37R9), prob=0.075\n",
      "4: src=google/gemma-7B-it(3dM7), dst=vivo-ai/BlueLM-7B-Chat(3mpH), prob=0.063\n",
      "5: src=google/gemma-7B-it(3dM7), dst=google/gemma-7B-it(742G), prob=0.475\n",
      "6: src=google/gemma-7B-it(3dM7), dst=vivo-ai/BlueLM-7B-Chat(6mMZ), prob=0.774\n",
      "7: src=vivo-ai/BlueLM-7B-Chat(32b8), dst=google/gemma-7B-it(3dM7), prob=0.351\n",
      "8: src=vivo-ai/BlueLM-7B-Chat(32b8), dst=google/gemma-7B-it(7Faz), prob=0.752\n",
      "9: src=vivo-ai/BlueLM-7B-Chat(32b8), dst=vivo-ai/BlueLM-7B-Chat(7gQh), prob=0.570\n",
      "10: src=vivo-ai/BlueLM-7B-Chat(32b8), dst=google/gemma-7B-it(37R9), prob=0.985\n",
      "11: src=vivo-ai/BlueLM-7B-Chat(32b8), dst=vivo-ai/BlueLM-7B-Chat(3mpH), prob=0.279\n",
      "12: src=vivo-ai/BlueLM-7B-Chat(32b8), dst=google/gemma-7B-it(742G), prob=0.921\n",
      "13: src=vivo-ai/BlueLM-7B-Chat(32b8), dst=vivo-ai/BlueLM-7B-Chat(6mMZ), prob=0.353\n",
      "14: src=google/gemma-7B-it(7Faz), dst=google/gemma-7B-it(3dM7), prob=0.289\n",
      "15: src=google/gemma-7B-it(7Faz), dst=vivo-ai/BlueLM-7B-Chat(32b8), prob=0.131\n",
      "16: src=google/gemma-7B-it(7Faz), dst=vivo-ai/BlueLM-7B-Chat(7gQh), prob=0.487\n",
      "17: src=google/gemma-7B-it(7Faz), dst=google/gemma-7B-it(37R9), prob=0.162\n",
      "18: src=google/gemma-7B-it(7Faz), dst=vivo-ai/BlueLM-7B-Chat(3mpH), prob=0.013\n",
      "19: src=google/gemma-7B-it(7Faz), dst=google/gemma-7B-it(742G), prob=0.780\n",
      "20: src=google/gemma-7B-it(7Faz), dst=vivo-ai/BlueLM-7B-Chat(6mMZ), prob=0.223\n",
      "21: src=vivo-ai/BlueLM-7B-Chat(7gQh), dst=google/gemma-7B-it(3dM7), prob=0.825\n",
      "22: src=vivo-ai/BlueLM-7B-Chat(7gQh), dst=vivo-ai/BlueLM-7B-Chat(32b8), prob=0.060\n",
      "23: src=vivo-ai/BlueLM-7B-Chat(7gQh), dst=google/gemma-7B-it(7Faz), prob=0.984\n",
      "24: src=vivo-ai/BlueLM-7B-Chat(7gQh), dst=google/gemma-7B-it(37R9), prob=0.180\n",
      "25: src=vivo-ai/BlueLM-7B-Chat(7gQh), dst=vivo-ai/BlueLM-7B-Chat(3mpH), prob=0.809\n",
      "26: src=vivo-ai/BlueLM-7B-Chat(7gQh), dst=google/gemma-7B-it(742G), prob=0.893\n",
      "27: src=vivo-ai/BlueLM-7B-Chat(7gQh), dst=vivo-ai/BlueLM-7B-Chat(6mMZ), prob=0.293\n",
      "28: src=google/gemma-7B-it(37R9), dst=google/gemma-7B-it(3dM7), prob=0.148\n",
      "29: src=google/gemma-7B-it(37R9), dst=vivo-ai/BlueLM-7B-Chat(32b8), prob=0.953\n",
      "30: src=google/gemma-7B-it(37R9), dst=google/gemma-7B-it(7Faz), prob=0.864\n",
      "31: src=google/gemma-7B-it(37R9), dst=vivo-ai/BlueLM-7B-Chat(7gQh), prob=0.063\n",
      "32: src=google/gemma-7B-it(37R9), dst=vivo-ai/BlueLM-7B-Chat(3mpH), prob=0.793\n",
      "33: src=google/gemma-7B-it(37R9), dst=google/gemma-7B-it(742G), prob=0.783\n",
      "34: src=google/gemma-7B-it(37R9), dst=vivo-ai/BlueLM-7B-Chat(6mMZ), prob=0.982\n",
      "35: src=vivo-ai/BlueLM-7B-Chat(3mpH), dst=google/gemma-7B-it(3dM7), prob=0.987\n",
      "36: src=vivo-ai/BlueLM-7B-Chat(3mpH), dst=vivo-ai/BlueLM-7B-Chat(32b8), prob=0.669\n",
      "37: src=vivo-ai/BlueLM-7B-Chat(3mpH), dst=google/gemma-7B-it(7Faz), prob=0.742\n",
      "38: src=vivo-ai/BlueLM-7B-Chat(3mpH), dst=vivo-ai/BlueLM-7B-Chat(7gQh), prob=0.006\n",
      "39: src=vivo-ai/BlueLM-7B-Chat(3mpH), dst=google/gemma-7B-it(37R9), prob=0.816\n",
      "40: src=vivo-ai/BlueLM-7B-Chat(3mpH), dst=google/gemma-7B-it(742G), prob=0.259\n",
      "41: src=vivo-ai/BlueLM-7B-Chat(3mpH), dst=vivo-ai/BlueLM-7B-Chat(6mMZ), prob=0.802\n",
      "42: src=google/gemma-7B-it(742G), dst=google/gemma-7B-it(3dM7), prob=0.134\n",
      "43: src=google/gemma-7B-it(742G), dst=vivo-ai/BlueLM-7B-Chat(32b8), prob=0.165\n",
      "44: src=google/gemma-7B-it(742G), dst=google/gemma-7B-it(7Faz), prob=0.260\n",
      "45: src=google/gemma-7B-it(742G), dst=vivo-ai/BlueLM-7B-Chat(7gQh), prob=0.039\n",
      "46: src=google/gemma-7B-it(742G), dst=google/gemma-7B-it(37R9), prob=0.933\n",
      "47: src=google/gemma-7B-it(742G), dst=vivo-ai/BlueLM-7B-Chat(3mpH), prob=0.241\n",
      "48: src=google/gemma-7B-it(742G), dst=vivo-ai/BlueLM-7B-Chat(6mMZ), prob=0.932\n",
      "49: src=vivo-ai/BlueLM-7B-Chat(6mMZ), dst=google/gemma-7B-it(3dM7), prob=0.342\n",
      "50: src=vivo-ai/BlueLM-7B-Chat(6mMZ), dst=vivo-ai/BlueLM-7B-Chat(32b8), prob=0.986\n",
      "51: src=vivo-ai/BlueLM-7B-Chat(6mMZ), dst=google/gemma-7B-it(7Faz), prob=0.892\n",
      "52: src=vivo-ai/BlueLM-7B-Chat(6mMZ), dst=vivo-ai/BlueLM-7B-Chat(7gQh), prob=0.177\n",
      "53: src=vivo-ai/BlueLM-7B-Chat(6mMZ), dst=google/gemma-7B-it(37R9), prob=0.434\n",
      "54: src=vivo-ai/BlueLM-7B-Chat(6mMZ), dst=vivo-ai/BlueLM-7B-Chat(3mpH), prob=0.023\n",
      "55: src=vivo-ai/BlueLM-7B-Chat(6mMZ), dst=google/gemma-7B-it(742G), prob=0.003\n",
      "56: src=google/gemma-7B-it(3dM7), dst=FinalDecision(8got), prob=0.792\n",
      "57: src=vivo-ai/BlueLM-7B-Chat(32b8), dst=FinalDecision(8got), prob=0.519\n",
      "58: src=google/gemma-7B-it(7Faz), dst=FinalDecision(8got), prob=0.629\n",
      "59: src=vivo-ai/BlueLM-7B-Chat(7gQh), dst=FinalDecision(8got), prob=0.556\n",
      "60: src=google/gemma-7B-it(37R9), dst=FinalDecision(8got), prob=0.460\n",
      "61: src=vivo-ai/BlueLM-7B-Chat(3mpH), dst=FinalDecision(8got), prob=0.938\n",
      "62: src=google/gemma-7B-it(742G), dst=FinalDecision(8got), prob=0.059\n",
      "63: src=vivo-ai/BlueLM-7B-Chat(6mMZ), dst=FinalDecision(8got), prob=0.852\n",
      "Variance: \n",
      "0: src=google/gemma-7B-it(3dM7), dst=vivo-ai/BlueLM-7B-Chat(32b8), prob=0.001\n",
      "1: src=google/gemma-7B-it(3dM7), dst=google/gemma-7B-it(7Faz), prob=0.028\n",
      "2: src=google/gemma-7B-it(3dM7), dst=vivo-ai/BlueLM-7B-Chat(7gQh), prob=0.050\n",
      "3: src=google/gemma-7B-it(3dM7), dst=google/gemma-7B-it(37R9), prob=0.009\n",
      "4: src=google/gemma-7B-it(3dM7), dst=vivo-ai/BlueLM-7B-Chat(3mpH), prob=0.010\n",
      "5: src=google/gemma-7B-it(3dM7), dst=google/gemma-7B-it(742G), prob=0.050\n",
      "6: src=google/gemma-7B-it(3dM7), dst=vivo-ai/BlueLM-7B-Chat(6mMZ), prob=0.041\n",
      "7: src=vivo-ai/BlueLM-7B-Chat(32b8), dst=google/gemma-7B-it(3dM7), prob=0.042\n",
      "8: src=vivo-ai/BlueLM-7B-Chat(32b8), dst=google/gemma-7B-it(7Faz), prob=0.046\n",
      "9: src=vivo-ai/BlueLM-7B-Chat(32b8), dst=vivo-ai/BlueLM-7B-Chat(7gQh), prob=0.052\n",
      "10: src=vivo-ai/BlueLM-7B-Chat(32b8), dst=google/gemma-7B-it(37R9), prob=0.003\n",
      "11: src=vivo-ai/BlueLM-7B-Chat(32b8), dst=vivo-ai/BlueLM-7B-Chat(3mpH), prob=0.033\n",
      "12: src=vivo-ai/BlueLM-7B-Chat(32b8), dst=google/gemma-7B-it(742G), prob=0.020\n",
      "13: src=vivo-ai/BlueLM-7B-Chat(32b8), dst=vivo-ai/BlueLM-7B-Chat(6mMZ), prob=0.055\n",
      "14: src=google/gemma-7B-it(7Faz), dst=google/gemma-7B-it(3dM7), prob=0.038\n",
      "15: src=google/gemma-7B-it(7Faz), dst=vivo-ai/BlueLM-7B-Chat(32b8), prob=0.026\n",
      "16: src=google/gemma-7B-it(7Faz), dst=vivo-ai/BlueLM-7B-Chat(7gQh), prob=0.031\n",
      "17: src=google/gemma-7B-it(7Faz), dst=google/gemma-7B-it(37R9), prob=0.022\n",
      "18: src=google/gemma-7B-it(7Faz), dst=vivo-ai/BlueLM-7B-Chat(3mpH), prob=0.002\n",
      "19: src=google/gemma-7B-it(7Faz), dst=google/gemma-7B-it(742G), prob=0.036\n",
      "20: src=google/gemma-7B-it(7Faz), dst=vivo-ai/BlueLM-7B-Chat(6mMZ), prob=0.031\n",
      "21: src=vivo-ai/BlueLM-7B-Chat(7gQh), dst=google/gemma-7B-it(3dM7), prob=0.030\n",
      "22: src=vivo-ai/BlueLM-7B-Chat(7gQh), dst=vivo-ai/BlueLM-7B-Chat(32b8), prob=0.011\n",
      "23: src=vivo-ai/BlueLM-7B-Chat(7gQh), dst=google/gemma-7B-it(7Faz), prob=0.003\n",
      "24: src=vivo-ai/BlueLM-7B-Chat(7gQh), dst=google/gemma-7B-it(37R9), prob=0.024\n",
      "25: src=vivo-ai/BlueLM-7B-Chat(7gQh), dst=vivo-ai/BlueLM-7B-Chat(3mpH), prob=0.049\n",
      "26: src=vivo-ai/BlueLM-7B-Chat(7gQh), dst=google/gemma-7B-it(742G), prob=0.014\n",
      "27: src=vivo-ai/BlueLM-7B-Chat(7gQh), dst=vivo-ai/BlueLM-7B-Chat(6mMZ), prob=0.034\n",
      "28: src=google/gemma-7B-it(37R9), dst=google/gemma-7B-it(3dM7), prob=0.035\n",
      "29: src=google/gemma-7B-it(37R9), dst=vivo-ai/BlueLM-7B-Chat(32b8), prob=0.009\n",
      "30: src=google/gemma-7B-it(37R9), dst=google/gemma-7B-it(7Faz), prob=0.027\n",
      "31: src=google/gemma-7B-it(37R9), dst=vivo-ai/BlueLM-7B-Chat(7gQh), prob=0.008\n",
      "32: src=google/gemma-7B-it(37R9), dst=vivo-ai/BlueLM-7B-Chat(3mpH), prob=0.022\n",
      "33: src=google/gemma-7B-it(37R9), dst=google/gemma-7B-it(742G), prob=0.030\n",
      "34: src=google/gemma-7B-it(37R9), dst=vivo-ai/BlueLM-7B-Chat(6mMZ), prob=0.004\n",
      "35: src=vivo-ai/BlueLM-7B-Chat(3mpH), dst=google/gemma-7B-it(3dM7), prob=0.003\n",
      "36: src=vivo-ai/BlueLM-7B-Chat(3mpH), dst=vivo-ai/BlueLM-7B-Chat(32b8), prob=0.037\n",
      "37: src=vivo-ai/BlueLM-7B-Chat(3mpH), dst=google/gemma-7B-it(7Faz), prob=0.034\n",
      "38: src=vivo-ai/BlueLM-7B-Chat(3mpH), dst=vivo-ai/BlueLM-7B-Chat(7gQh), prob=0.001\n",
      "39: src=vivo-ai/BlueLM-7B-Chat(3mpH), dst=google/gemma-7B-it(37R9), prob=0.030\n",
      "40: src=vivo-ai/BlueLM-7B-Chat(3mpH), dst=google/gemma-7B-it(742G), prob=0.042\n",
      "41: src=vivo-ai/BlueLM-7B-Chat(3mpH), dst=vivo-ai/BlueLM-7B-Chat(6mMZ), prob=0.028\n",
      "42: src=google/gemma-7B-it(742G), dst=google/gemma-7B-it(3dM7), prob=0.020\n",
      "43: src=google/gemma-7B-it(742G), dst=vivo-ai/BlueLM-7B-Chat(32b8), prob=0.043\n",
      "44: src=google/gemma-7B-it(742G), dst=google/gemma-7B-it(7Faz), prob=0.039\n",
      "45: src=google/gemma-7B-it(742G), dst=vivo-ai/BlueLM-7B-Chat(7gQh), prob=0.006\n",
      "46: src=google/gemma-7B-it(742G), dst=google/gemma-7B-it(37R9), prob=0.013\n",
      "47: src=google/gemma-7B-it(742G), dst=vivo-ai/BlueLM-7B-Chat(3mpH), prob=0.036\n",
      "48: src=google/gemma-7B-it(742G), dst=vivo-ai/BlueLM-7B-Chat(6mMZ), prob=0.012\n",
      "49: src=vivo-ai/BlueLM-7B-Chat(6mMZ), dst=google/gemma-7B-it(3dM7), prob=0.043\n",
      "50: src=vivo-ai/BlueLM-7B-Chat(6mMZ), dst=vivo-ai/BlueLM-7B-Chat(32b8), prob=0.002\n",
      "51: src=vivo-ai/BlueLM-7B-Chat(6mMZ), dst=google/gemma-7B-it(7Faz), prob=0.018\n",
      "52: src=vivo-ai/BlueLM-7B-Chat(6mMZ), dst=vivo-ai/BlueLM-7B-Chat(7gQh), prob=0.027\n",
      "53: src=vivo-ai/BlueLM-7B-Chat(6mMZ), dst=google/gemma-7B-it(37R9), prob=0.043\n",
      "54: src=vivo-ai/BlueLM-7B-Chat(6mMZ), dst=vivo-ai/BlueLM-7B-Chat(3mpH), prob=0.005\n",
      "55: src=vivo-ai/BlueLM-7B-Chat(6mMZ), dst=google/gemma-7B-it(742G), prob=0.001\n",
      "56: src=google/gemma-7B-it(3dM7), dst=FinalDecision(8got), prob=0.032\n",
      "57: src=vivo-ai/BlueLM-7B-Chat(32b8), dst=FinalDecision(8got), prob=0.040\n",
      "58: src=google/gemma-7B-it(7Faz), dst=FinalDecision(8got), prob=0.041\n",
      "59: src=vivo-ai/BlueLM-7B-Chat(7gQh), dst=FinalDecision(8got), prob=0.060\n",
      "60: src=google/gemma-7B-it(37R9), dst=FinalDecision(8got), prob=0.049\n",
      "61: src=vivo-ai/BlueLM-7B-Chat(3mpH), dst=FinalDecision(8got), prob=0.015\n",
      "62: src=google/gemma-7B-it(742G), dst=FinalDecision(8got), prob=0.012\n",
      "63: src=vivo-ai/BlueLM-7B-Chat(6mMZ), dst=FinalDecision(8got), prob=0.019\n"
     ]
    }
   ],
   "source": [
    "print(all_edge_probs[:][0])\n",
    "all_probs_variance = torch.std(all_edge_probs, dim=0)\n",
    "print(all_probs_variance)\n",
    "all_probs_mean = torch.mean(all_edge_probs, dim=0)\n",
    "print(all_probs_variance.shape)\n",
    "print(all_probs_mean.shape)\n",
    "print(\"Mean: \")\n",
    "_print_conns(all_probs_mean, swarm)\n",
    "print(\"Variance: \")\n",
    "_print_conns(all_probs_variance, swarm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean: \n",
    "0: src=DirectAnswer(5Q3h), dst=CoTStep(6RLA), prob=0.997\n",
    "1: src=DirectAnswer(5Q3h), dst=CoTStep(6Y3D), prob=0.001\n",
    "2: src=DirectAnswer(5Q3h), dst=CoTStep(8y3C), prob=0.646\n",
    "3: src=CoTStep(6RLA), dst=DirectAnswer(5Q3h), prob=0.001\n",
    "4: src=CoTStep(6Y3D), dst=DirectAnswer(5Q3h), prob=0.001\n",
    "5: src=CoTStep(8y3C), dst=DirectAnswer(5Q3h), prob=0.000\n",
    "6: src=DirectAnswer(5Q3h), dst=FinalDecision(7fFJ), prob=0.988\n",
    "7: src=CoTStep(6RLA), dst=FinalDecision(7fFJ), prob=0.169\n",
    "8: src=CoTStep(6Y3D), dst=FinalDecision(7fFJ), prob=0.004\n",
    "9: src=CoTStep(8y3C), dst=FinalDecision(7fFJ), prob=0.918\n",
    "Variance: \n",
    "0: src=DirectAnswer(5Q3h), dst=CoTStep(6RLA), prob=0.000\n",
    "1: src=DirectAnswer(5Q3h), dst=CoTStep(6Y3D), prob=0.000\n",
    "2: src=DirectAnswer(5Q3h), dst=CoTStep(8y3C), prob=0.000\n",
    "3: src=CoTStep(6RLA), dst=DirectAnswer(5Q3h), prob=0.000\n",
    "4: src=CoTStep(6Y3D), dst=DirectAnswer(5Q3h), prob=0.000\n",
    "5: src=CoTStep(8y3C), dst=DirectAnswer(5Q3h), prob=0.000\n",
    "6: src=DirectAnswer(5Q3h), dst=FinalDecision(7fFJ), prob=0.000\n",
    "7: src=CoTStep(6RLA), dst=FinalDecision(7fFJ), prob=0.000\n",
    "8: src=CoTStep(6Y3D), dst=FinalDecision(7fFJ), prob=0.000\n",
    "9: src=CoTStep(8y3C), dst=FinalDecision(7fFJ), prob=0.000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EdgeWiseDistributionByModel' object has no attribute 'edge_logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m edge_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[43mswarm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnection_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_logits\u001b[49m)\n\u001b[1;32m      2\u001b[0m _print_conns(edge_probs, swarm)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#edge_probs = all_probs_mean\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#reorder the input\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#edge_probs = [*edge_probs[:-8], *edge_probs[-8::2], *edge_probs[-7::2]]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gptswarm/lib/python3.10/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EdgeWiseDistributionByModel' object has no attribute 'edge_logits'"
     ]
    }
   ],
   "source": [
    "edge_probs = torch.sigmoid(swarm.connection_dist.edge_logits)\n",
    "_print_conns(edge_probs, swarm)\n",
    "#edge_probs = all_probs_mean\n",
    "#reorder the input\n",
    "#edge_probs = [*edge_probs[:-8], *edge_probs[-8::2], *edge_probs[-7::2]]\n",
    "adjacency_matrix,node_indices,names = create_adjacency_matrix(edge_probs, swarm)\n",
    "#reorder the matrix\n",
    "print((adjacency_matrix[1::2,:]).shape)\n",
    "adjacency_matrix = np.concatenate([adjacency_matrix[0,:][np.newaxis,:], adjacency_matrix[1::2,:], adjacency_matrix[2::2,:]], 0)\n",
    "adjacency_matrix = np.concatenate([adjacency_matrix[:,0][:,np.newaxis], adjacency_matrix[:,1::2], adjacency_matrix[:,2::2]], 1)\n",
    "_print_conns(edge_probs, swarm)\n",
    "# Plot the heatmap with a smoother colormap\n",
    "fig, ax = plt.subplots()\n",
    "heatmap = ax.imshow(adjacency_matrix, cmap='viridis')\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = plt.colorbar(heatmap)\n",
    "\n",
    "# Set the tick labels\n",
    "num_nodes = len(node_indices)\n",
    "ax.set_xticks(np.arange(num_nodes))\n",
    "ax.set_yticks(np.arange(num_nodes))\n",
    "ax.set_xticklabels([str(node_id) for node_id in node_indices.values()])\n",
    "ax.set_yticklabels([str(node_id) for node_id in node_indices.values()])\n",
    "\n",
    "# Rotate the tick labels if needed\n",
    "plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\")\n",
    "\n",
    "# Set the axis labels\n",
    "ax.set_xlabel('Destination Node')\n",
    "ax.set_ylabel('Source Node')\n",
    "\n",
    "# Set the title\n",
    "ax.set_title('Probability Heatmap')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "for node in node_indices:\n",
    "    print(node_indices[node], names[node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_matrix = np.stack([adjacency_matrix[0::2,:], adjacency_matrix[1::2,:]])\n",
    "adjacency_matrix = np.stack([adjacency_matrix[:,0::2], adjacency_matrix[:,1::2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_matrix = np.stack([adjacency_matrix[0::2,:], adjacency_matrix[1::2,:]])\n",
    "adjacency_matrix = np.stack([adjacency_matrix[:,0::2], adjacency_matrix[:,1::2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_matrix = np.stack([adjacency_matrix[0::2,:], adjacency_matrix[1::2,:]])\n",
    "adjacency_matrix = np.stack([adjacency_matrix[:,0::2], adjacency_matrix[:,1::2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(swarm.connection_dist.model.linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(model=\"google/gemma-7B-it\", dtype=\"half\", max_model_len=5888)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7B-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [{\"role\": \"user\", \"content\": \"Write a poem about cats.\"}]\n",
    "prompt = tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, top_k=50,max_tokens=2000)\n",
    "outputs = llm.generate(prompt,sampling_params=sampling_params)\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install accelerate\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7B-it\")\n",
    "#model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7B-it\",torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7B-it\", torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tokenizer.special_tokens_map)\n",
    "#dummy message\n",
    "message = [{\"role\": \"user\", \"content\": \"Write a poem about cats.\"}]\n",
    "prompt = tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n",
    "prompt = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda\")\n",
    "prompt_len = len(prompt[0])\n",
    "outputs = model.generate(\n",
    "            prompt,\n",
    "            do_sample=True,\n",
    "            max_length=2000,\n",
    "            top_k=50,\n",
    "            top_p=1.0\n",
    "        )\n",
    "output_text = tokenizer.decode(outputs[0][prompt.shape[-1]:],skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the GPTChat class\n",
    "gpt_chat = CustomLLM()\n",
    "#gpt_chat_2 = CustomLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#if gpt_chat is gpt_chat_2:\n",
    "#    print(\"Same instance\")\n",
    "# Create a list of Message objects\n",
    "messages = [Message(role=\"user\", content=\"What would a dog say if he could speak?\")]\n",
    "# Move messages to GPU\n",
    "# Create tasks for the gen method\n",
    "tasks = [asyncio.create_task(gpt_chat.agen(messages))]#, asyncio.create_task(gpt_chat_2.agen(messages))]\n",
    "\n",
    "# Wait for the tasks to complete and get the results\n",
    "results = [await task for task in tasks]\n",
    "\n",
    "# Print the output\n",
    "print(results)\n",
    "#print(results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_connections = [0 for _ in range(188)]\n",
    "init_connection_probability = 0.1\n",
    "domain = \"crosswords\"\n",
    "llm_backbone_name=\"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swarm = Swarm([\"CrosswordsReflection\", \"CrosswordsToT\", \"CrosswordsBruteForceOpt\"], \"crosswords\", \"gpt-3.5-turbo-1106\", #\"gpt-4-1106-preview\"\n",
    "            final_node_class=\"ReturnAll\", \n",
    "            final_node_kwargs={},\n",
    "            edge_optimize=True,\n",
    "            init_connection_probability=init_connection_probability, \n",
    "            connect_output_nodes_to_final_node=connect_output_nodes_to_final_node, \n",
    "            include_inner_agent_connections=include_inner_agent_connections,\n",
    "            edge_network_enable=edge_network_enable,\n",
    "            llm_backbone_name=llm_backbone_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_network = EdgeNetwork(llm_backbone_name=llm_backbone_name, num_edges=len(potential_connections), initial_probability=init_connection_probability)\n",
    "connection_dist = EdgeWiseDistributionByModel(potential_connections, edge_network, domain)\n",
    "\n",
    "connection_dist.load_state_dict(torch.load(\"result/crosswords/experiment_edge_logits_10.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of EdgeNetwork\n",
    "num_edges = 188  # Replace with the number of edges used when saving the model\n",
    "llm_backbone_name = 'gpt2'  # Replace with the name of the model used when saving the model\n",
    "model = EdgeNetwork(llm_backbone_name, num_edges)\n",
    "\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load('result/crosswords/experiment_edge_logits_10.pt')\n",
    "\n",
    "# Remove 'model.' prefix from state dictionary keys and exclude 'order_params'\n",
    "state_dict = {k.replace('model.', ''): v for k, v in state_dict.items() if k != 'model.order_params'}\n",
    "\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of EdgeNetwork\n",
    "num_edges = 188  # Replace with the number of edges used when saving the model\n",
    "llm_backbone_name = 'gpt2'  # Replace with the name of the model used when saving the model\n",
    "model = EdgeNetwork(llm_backbone_name, num_edges)\n",
    "\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load('result/crosswords/experiment_edge_logits_10.pt')\n",
    "\n",
    "# Remove 'model.' prefix from state dictionary keys\n",
    "state_dict = {k.replace('model.', ''): v for k, v in state_dict.items()}\n",
    "\n",
    "model.load_state_dict(state_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weak_to_strong",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
